{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ae5eaa1",
   "metadata": {},
   "source": [
    "## Module 1: NER and POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8756082",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tay Han\\OneDrive - National University of Singapore\\Capstone\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import uuid\n",
    "import json\n",
    "from typing import List, Dict, Any, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "import dateparser\n",
    "import google.generativeai as genai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4431ec6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMINI_API_KEY = \"AIzaSyDwNbpu5hvUJ2wZYXMY14zQlsevLdlr2qw\"\n",
    "genai.configure(api_key=GEMINI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7be767e",
   "metadata": {},
   "source": [
    "Creation of the Entity Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bc1a6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Entity(BaseModel):\n",
    "    text: str\n",
    "    label: str\n",
    "    start: int\n",
    "    end: int\n",
    "    confidence: float = 1.0\n",
    "\n",
    "\n",
    "class EntityCollection(BaseModel):\n",
    "    conversation_id: str\n",
    "    timestamp: str\n",
    "    raw_text: str\n",
    "    sentences: List[str]\n",
    "    entities: List[Entity]\n",
    "\n",
    "    def to_endpoint_json(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"conversation_id\": self.conversation_id,\n",
    "            \"input\": [{\"role\": \"user\", \"content\": self.raw_text}],\n",
    "            \"sentences\": self.sentences,\n",
    "            \"entities\": [e.dict() for e in self.entities],\n",
    "        }\n",
    "\n",
    "\n",
    "class FinalStructuredOutput(BaseModel):\n",
    "    \"\"\"This is the final JSON we expect after the LLM post-processor.\"\"\"\n",
    "    conversation_id: str\n",
    "    input: List[Dict[str, Any]]\n",
    "    sentences: List[str]\n",
    "    entities: List[Entity]\n",
    "    # Optional, but useful for downstream routing\n",
    "    contextual_tags: Optional[Dict[str, Any]] = Field(default=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15de21a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    \"\"\"Cleans elderly speech before NLP: casing, fillers, en-SG removal, basic typo-normalisation.\"\"\"\n",
    "\n",
    "    _FILLERS = [\n",
    "        r\"\\buh+\\b\", r\"\\bum+\\b\", r\"\\bah+\\b\", r\"\\bhmm+\\b\", r\"\\byou know\\b\",\n",
    "        r\"\\blike\\b\", r\"\\bkind of\\b\", r\"\\bsort of\\b\",\n",
    "    ]\n",
    "    _EN_SG = [r\"\\blah\\b\", r\"\\bleh\\b\", r\"\\blor\\b\", r\"\\bmeh\\b\", r\"\\bsia\\b\", r\"\\bhor\\b\"]\n",
    "\n",
    "    def __init__(self, keep_case: bool = True):\n",
    "        self.keep_case = keep_case\n",
    "\n",
    "    def sentence_segment(self, text: str) -> List[str]:\n",
    "        segs = re.split(r\"(?<=[\\.\\!\\?])\\s+\", text.strip())\n",
    "        segs = [s.strip() for s in segs if s.strip()]\n",
    "        return segs if segs else [text.strip()]\n",
    "\n",
    "    def normalize(self, text: str) -> str:\n",
    "        t = text.strip()\n",
    "        if not self.keep_case:\n",
    "            t = t.lower()\n",
    "        for pat in self._FILLERS + self._EN_SG:\n",
    "            t = re.sub(pat, \"\", t, flags=re.IGNORECASE)\n",
    "        t = re.sub(r\"\\bI\\'m\\b\", \"I am\", t)\n",
    "        t = re.sub(r\"\\bcan\\'t\\b\", \"cannot\", t)\n",
    "        t = re.sub(r\"\\bwon\\'t\\b\", \"will not\", t)\n",
    "        # Collapse spaces\n",
    "        t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "        return t\n",
    "\n",
    "    def process(self, text: str) -> Dict[str, Any]:\n",
    "        cleaned = self.normalize(text)\n",
    "        sentences = self.sentence_segment(cleaned)\n",
    "        return {\"cleaned\": cleaned, \"sentences\": sentences}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13bf2ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NER_POS_Extractor:\n",
    "    \"\"\"\n",
    "    Extracts both NER (semantic entities like DATE, ORG, EVENT) \n",
    "    and POS (grammatical roles like VERB, NOUN, etc.) and embeds the POS tags\n",
    "    into the sentence for easier processing by downstream models (e.g., Gemini).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: str = \"en_core_web_lg\"):\n",
    "        self.nlp = spacy.load(model)\n",
    "\n",
    "    def embed_pos_in_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Embeds POS tags directly next to the word (e.g., I <NOUN> love <VERB> playing).\n",
    "        Also captures verbs as <ACTION>.\n",
    "        \"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        pos_embedded_text = text\n",
    "\n",
    "        # Loop over the tokens and insert POS tags after each token\n",
    "        for token in reversed(doc):\n",
    "            pos_tag = token.pos_  # Extract the POS tag (e.g., \"NOUN\", \"VERB\")\n",
    "            token_text = token.text\n",
    "\n",
    "            # If the POS tag is a verb, mark it as an action\n",
    "            if pos_tag == \"VERB\":\n",
    "                pos_tag = \"ACTION\"  # Custom tag for actions (verbs)\n",
    "\n",
    "            # Insert POS tag directly after the word in the text\n",
    "            pos_embedded_text = (\n",
    "                pos_embedded_text[:token.idx + len(token_text)]  # Text up to the token\n",
    "                + f\" <{pos_tag}>\"  # POS tag after the word (or ACTION for verbs)\n",
    "                + pos_embedded_text[token.idx + len(token_text):]  # Text after the token\n",
    "            )\n",
    "\n",
    "        return pos_embedded_text\n",
    "\n",
    "    def extract(self, cleaned_text: str, sentences: List[str]) -> EntityCollection:\n",
    "        doc = self.nlp(cleaned_text)\n",
    "        entities: List[Entity] = []\n",
    "\n",
    "        # -------------------------------\n",
    "        # 1) NER: Semantic entity labels\n",
    "        # -------------------------------\n",
    "        for ent in doc.ents:\n",
    "            entities.append(\n",
    "                Entity(\n",
    "                    text=ent.text,\n",
    "                    label=f\"NER_{ent.label_}\",  # make it clear this came from NER\n",
    "                    start=ent.start_char,\n",
    "                    end=ent.end_char,\n",
    "                    confidence=1.0\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # -------------------------------\n",
    "        # 2) Embed POS tags into the text for downstream models like Gemini\n",
    "        # -------------------------------\n",
    "        pos_embedded_text = self.embed_pos_in_text(cleaned_text)\n",
    "\n",
    "        # -------------------------------\n",
    "        # 3) Wrap results in EntityCollection\n",
    "        # -------------------------------\n",
    "        return EntityCollection(\n",
    "            conversation_id=str(uuid.uuid4()),\n",
    "            timestamp=datetime.utcnow().isoformat(),\n",
    "            raw_text=pos_embedded_text,  # The processed text with POS tags embedded\n",
    "            sentences=sentences,\n",
    "            entities=entities\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8fe7c856",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeminiPostProcessor:\n",
    "    def __init__(self, api_key: str, model_name: str = \"gemini-2.5-flash-lite\"):\n",
    "        genai.configure(api_key=api_key)\n",
    "        self.model = genai.GenerativeModel(model_name)\n",
    "\n",
    "    def parse(self, collection: EntityCollection) -> Dict[str, Any]:\n",
    "        print(collection.raw_text)\n",
    "        system_prompt = \"\"\"You are an assistant that converts elderly conversation into structured JSON.\n",
    "          The language used will also be singlish based, attempt to understand the singlish nuances within the statements.\n",
    "          The schema must look like this:\n",
    "\n",
    "          {\n",
    "            \"conversation_id\": \"...\",\n",
    "            \"input\": [{\"role\": \"user\", \"content\": \"...\"}],\n",
    "            \"sentences\": [...],\n",
    "            \"entities\": [\n",
    "              {\"text\": \"...\", \"label\": \"ACTIVITY/FOOD/DATE/EVENT/FACILITY/ACTION\", \"start\": ..., \"end\": ...}\n",
    "            ]\n",
    "          }\n",
    "          \"\"\"\n",
    "\n",
    "        user_prompt = f\"\"\"\n",
    "          Conversation text (POS-tagged):\n",
    "          {collection.raw_text}\n",
    "\n",
    "          NER hints:\n",
    "          {json.dumps([e.dict() for e in collection.entities if e.label.startswith(\"NER_\")], indent=2)}\n",
    "\n",
    "          Action hints:\n",
    "          {json.dumps([e.dict() for e in collection.entities if e.label == \"ACTION\"], indent=2)}\n",
    "          \"\"\"\n",
    "\n",
    "        # Get response from the model\n",
    "        response = self.model.generate_content([system_prompt, user_prompt])\n",
    "        result_text = response.text.strip()\n",
    "\n",
    "        # --- Clean output ---\n",
    "        if result_text.startswith(\"```json\") and result_text.endswith(\"```\"):\n",
    "            result_text = result_text[7:-3].strip()  # Remove the ```json and closing ```\n",
    "\n",
    "        # 2. Extract the JSON content between braces {}\n",
    "        match = re.search(r\"\\{.*\\}\", result_text, re.DOTALL)\n",
    "        if match:\n",
    "            result_text = match.group(0)  # Extract valid JSON\n",
    "\n",
    "        # --- Try to parse the cleaned JSON ---\n",
    "        try:\n",
    "            return json.loads(result_text)\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Invalid JSON from Gemini: {e}\", \"raw\": result_text}\n",
    "\n",
    "    def embed_actions_in_entities(self, collection: EntityCollection) -> EntityCollection:\n",
    "        \"\"\"\n",
    "        Identifies all verbs in the raw text and adds them as entities with the label \"ACTION\".\n",
    "        \"\"\"\n",
    "        doc = self.model.nlp(collection.raw_text)\n",
    "        action_entities = []\n",
    "\n",
    "        # Identify verbs (actions) and create Entity objects for them\n",
    "        for token in doc:\n",
    "            if token.pos_ == \"VERB\":\n",
    "                action_entities.append(\n",
    "                    Entity(\n",
    "                        text=token.text,\n",
    "                        label=\"ACTION\",\n",
    "                        start=token.idx,\n",
    "                        end=token.idx + len(token.text)\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        # Add action entities to the existing collection of entities\n",
    "        collection.entities.extend(action_entities)\n",
    "        return collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a4aaff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaturalLanguageToJSONPipeline:\n",
    "    def __init__(self):\n",
    "        self.pre = TextPreprocessor()\n",
    "        self.extractor = NER_POS_Extractor()\n",
    "        self.llm = GeminiPostProcessor(api_key=GEMINI_API_KEY)\n",
    "\n",
    "    def run(self, text: str) -> Dict[str, Any]:\n",
    "        print(\"Beginnnign Preprocessing of texts\")\n",
    "        prep = self.pre.process(text)\n",
    "        print(\"done\")\n",
    "\n",
    "        cleaned, sentences = prep[\"cleaned\"], prep[\"sentences\"]\n",
    "        print(\"Starting NER POS Extraction Process\")\n",
    "        collection = self.extractor.extract(cleaned, sentences)\n",
    "        print(\"Done\")\n",
    "\n",
    "        print(\"Starting LLM Parse\")\n",
    "        structured = self.llm.parse(collection)\n",
    "        print(\"Done\")\n",
    "\n",
    "        try:\n",
    "            validated = FinalStructuredOutput(**structured)\n",
    "            return validated.dict()\n",
    "        except ValidationError as e:\n",
    "            return {\"error\": \"Validation failed\", \"details\": e.errors(), \"raw\": structured}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "adffddfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginnnign Preprocessing of texts\n",
      "done\n",
      "Starting NER POS Extraction Process\n",
      "Done\n",
      "Starting LLM Parse\n",
      "wa <INTJ> damn <INTJ> sian <PROPN> , <PUNCT> forgot <ACTION> to <PART> eat <ACTION> my <PRON> medicine <NOUN>\n",
      "Done\n",
      "{\n",
      "  \"conversation_id\": \"conv_001\",\n",
      "  \"input\": [\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"wa damn sian, forgot to eat my medicine\"\n",
      "    }\n",
      "  ],\n",
      "  \"sentences\": [\n",
      "    \"wa damn sian, forgot to eat my medicine\"\n",
      "  ],\n",
      "  \"entities\": [\n",
      "    {\n",
      "      \"text\": \"forgot to eat\",\n",
      "      \"label\": \"ACTION\",\n",
      "      \"start\": 14,\n",
      "      \"end\": 27,\n",
      "      \"confidence\": 1.0\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"medicine\",\n",
      "      \"label\": \"FOOD\",\n",
      "      \"start\": 31,\n",
      "      \"end\": 39,\n",
      "      \"confidence\": 1.0\n",
      "    }\n",
      "  ],\n",
      "  \"contextual_tags\": null\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tay Han\\AppData\\Local\\Temp\\ipykernel_32932\\1596187666.py:23: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  return validated.dict()\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example elderly conversation input\n",
    "    text = (\n",
    "        \"wa damn sian sia, forgot to eat my medicine sia\"\n",
    "    )\n",
    "\n",
    "    # Build pipeline\n",
    "    pipeline = NaturalLanguageToJSONPipeline()\n",
    "\n",
    "    # Run pipeline on input text\n",
    "    output = pipeline.run(text)\n",
    "\n",
    "    # Pretty-print JSON output\n",
    "    print(json.dumps(output, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fe9b377",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2410008431.py, line 2)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mpip install -U \"transformers>=4.44\" \"datasets>=2.20\" \"accelerate>=0.33\" \"peft>=0.11.0\" evaluate sacrebleu\u001b[39m\n        ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Environment setup\n",
    "pip install -U \"transformers>=4.44\" \"datasets>=2.20\" \"accelerate>=0.33\" \"peft>=0.11.0\" evaluate sacrebleu\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
